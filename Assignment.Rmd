---
title: Assignment One
---

```{r}
library(rpart)
library(dplyr)
df <- read.csv("~/Desktop/csen1061-assignment-modeling/Mines_Rocks.csv", header=FALSE)
colnames(df)[which(names(df) == "V61")] <- "class"
set.seed(42)
```


#Part 1 : Decision tree#


First CART was used to form the decision Tree using the entire dataset as the learning set  

```{r}
 cfit <- rpart(class ~.,data = df)
```

Then the classifier is tested on the same dataset which was used in training (the entire dataset).
And then the confusion matrix is obtained 

```{r}
predictions <- predict(cfit, df, type="class")
mat <- table(predictions, df$class)
mat
```

Now doing some evaluations : 

__Accuracy:__  Overall, how often is the classifier correct 
(TP+TN)/total = 
```{r}
(mat[1,"M"] + mat[2,"R"]) / 208
```

__Error:__ How often is the classifier incorrect 

```{r}
1- ((mat[1,"M"] + mat[2,"R"]) / 208)
```
__Precision:__  When it predicts "M", how often is it correct , TP = predicted "M" Right 
TP/(predicted "M") = 
```{r}
p <- mat[1,"M"] / (mat[1,"R"] + mat[1,"M"])
p
```

When it predicts "R",  how often is it correct  , TP = predicted "R" Right 
TP/(predicted "R") = 
```{r}
mat[2,"R"] / (mat[2,"R"] + mat[2,"M"])
```

__Recall:__  When it's actually "M", how often does it predict "M" , TP = predicted "M" Right 
TP/(actual "M" in the data )
```{r}
r <- mat[1,"M"] / (mat[1,"M"] + mat[2,"M"])
r
```

 When it's actually "R", how often does it predict "R" , TP = predicted "R" Right 
TP/(actual "R" in the data )
```{r}
mat[2,"R"] / (mat[1,"R"] + mat[2,"R"])
```

__F1_score:__ which is the harmonic mean of precision and recall assuming that this question was asked "Is it 'M'  ?"
F1 = 2pr/(p+r) = 
```{r}
2*p*r/(p+r)
```


Training and testing a classifier on the same data leads to over-fitting and false results (too opti-mistic).
Now we use the k-fold cross-validation to test the classifier where K = 10 . 

```{r}
 n <- nrow(df)
 K <- 10
 rowsInFold <- n %/% K
 rang <- rank(runif(n))
 block <- (rang-1) %/% rowsInFold +1
 block <- as.factor(block)
```

Now the block vector contains the the corresponding fold of each row and now we can proceed with the classification 


```{r}
 # variables to store the accuray precision recall and f1
 acc = numeric(0)
 p = numeric(0)
 r = numeric(0)
 f1 = numeric(0)
 
  for(k in 1:K) {
    # build classifier with all blocks that are not K 
     cfit <- rpart(class ~.,data = df[block!= k,])
    # predict the output of the k block 
     predictions <- predict(cfit, df[block == k,], type="class")
    # store all the neccessary evals to get their mean later 
     mat <- table(predictions, df$class[block == k])
     acc <- rbind(acc,(mat[1,"M"] + mat[2,"R"]) / (mat[1,"M"] + mat[2,"M"]+mat[1,"R"] + mat[2,"R"]))
     ptemp = mat[1,"M"] / (mat[1,"R"] + mat[1,"M"])
     p <- rbind(p,ptemp)
     rtemp = mat[1,"M"] / (mat[1,"M"] + mat[2,"M"])
     r <- rbind(r,rtemp)
     f1 <- rbind(f1,(2*ptemp*rtemp/(ptemp+rtemp)) )
 }
```

Now we find the mean of all of what we obtained so far

__Accuracy__ 
```{r}
mean(acc)
```

__Error__ 
```{r}
1-mean(acc)
```

__Precision__ 
```{r}
mean(p)
```

__Recall__ 
```{r}
mean(r)
```
__F1_Score__ 
```{r}
mean(f1)
```


As we can see the evaluation measures are worse after using the 10-fold validation as in the first case the whole data was used to build the classifier and the exact same data was used in prediction which is not the case when doing the 10-fold validation.

#Part 2#
 We can store the above cross validation code into a function inorder to avoid duplicating code as follow
 
```{r}
 cross_validate <- function(df,block,classifier)
 {
 acc = numeric(0)
 p = numeric(0)
 r = numeric(0)
 f1 = numeric(0)
 one = c("Accuracy","Error","Precision","Recall","F1_score")
 for(k in 1:K) {
     # build classifier with all blocks that are not K 
     cfit <- classifier(class ~.,data = df[block!= k,])
     # predict the output of the k block 
     predictions <- predict(cfit, df[block == k,], type="class")
     # store all the neccessary evals to get their mean later 
     mat <- table(predictions, df$class[block == k])
     acc <- rbind(acc,(mat[1,"M"] + mat[2,"R"]) / (mat[1,"M"] + mat[2,"M"]+mat[1,"R"] + mat[2,"R"]))
     ptemp = mat[1,"M"] / (mat[1,"R"] + mat[1,"M"])
     p <- rbind(p,ptemp)
     rtemp = mat[1,"M"] / (mat[1,"M"] + mat[2,"M"])
     r <- rbind(r,rtemp)
     f1 <- rbind(f1,(2*ptemp*rtemp/(ptemp+rtemp)) )
 }
 two = c(mean(acc),(1-mean(acc)), mean(p), mean(r), mean(f1))
 data.frame(one,two) 
 }
```

###__Random Forest__###

```{r}
 library(randomForest)
 cross_validate(df,block,randomForest)
```

###__Support Vector Machine__###

```{r}
  library(e1071)
cross_validate(df,block,svm)
```

###__Naive Bayes__###

```{r}
cross_validate(df,block,naiveBayes)
```


###__Neural Networks__###

```{r}
library("neuralnet")
```

For some reason the formula y~. is not accepted in the neuralnet() function. So we have to write the formula ourselves and the function cross_validate cant be called in this one  also i encountered a problem running the neural network classifier as the column of class is of type String and it should be an integer so i introduced a modified data frame to run the neural network classifier on  and rewrote the formula to be used with the classifier 
```{r}
 df_NN<- df
 df_NN$class2[df_NN$class == "R"] <- 0
 df_NN$class2[df_NN$class == "M"] <- 1
 df_NN$class<- df_NN$class2
 df_NN$class2 <- NULL
 n <- names(df_NN)
f <- as.formula(paste("class ~", paste(n[!n %in% "class"], collapse = " + ")))
```

Now it is possible to run the Neural network classifier and it will ran using 10-fold cross validation 
Experimenting with number of hidden layers here we set 2 hidden layers first one with 5 and 3 neurons each 

```{r}
 
cv.error <- numeric(0)

 for(k in 1:K) {
   train <- df_NN[block!= k,]
   test <- df_NN[block== k,]
     nn <- neuralnet(f,train,hidden=c(5,3),linear.output=T)
    pr.nn <- compute(nn,test[,1:60])
    pr.nn <- pr.nn$net.result*(max(df_NN$class)-min(df_NN$class))+min(df_NN$class)
    
    test.r <- (test$class)*(max(df_NN$class)-min(df_NN$class))+min(df_NN$class)
    
    cv.error <- rbind(cv.error,sum((test.r - pr.nn)^2)/nrow(test) ) 
 }
mean(cv.error)
 
```

#Part 3#

Now we import more data sets to test the algorithms more and try to check when one is better  and forming our matrix 

```{r}
hyp <- read.csv("~/Desktop/csen1061-assignment-modeling/hyp.csv", header=FALSE)
diab <- read.csv("~/Desktop/csen1061-assignment-modeling/diab.csv", header=FALSE)


error_table <- matrix(nrow=4, ncol=4)
rownames(error_table) <- c('Sonar','Hepatitis','Spect','Pima-indians')
colnames(error_table) <- c('Decision_Tree ','Random_Forest','Support_Vector_Machine','Naive_Bayes')

```

Now we need to run the algorithms on each dataset 10 times using 10-fold cross validation but with changing the content of the fold we will begin filling data of the first frame "Sonar" 

```{r}
 n <- nrow(df)
 K <- 10
 rowsInFold <- n %/% K
 rang <- rank(runif(n))
 block <- (rang-1) %/% rowsInFold +1
 block <- as.factor(block)
```

Now we will proceed to with 10 times of 10 fold cross validation by first randomizing the content of the block vector inorder to have different fold in each iteration of the 10 

```{r}
 total_error_one <- numeric(0)
  total_error_two <- numeric(0)
 total_error_three <- numeric(0)
 total_error_four <- numeric(0)

 for(k in 1:K) {
   sample(block)
   a <- cross_validate(df,block,rpart)
   b <- cross_validate(df,block,randomForest)
   c <- cross_validate(df,block,svm)
   d <- cross_validate(df,block,naiveBayes)

   total_error_one<- rbind(total_error_one, a$two[2])
    total_error_two<- rbind(total_error_two, b$two[2])
   total_error_three<- rbind(total_error_three, c$two[2])
   total_error_four<- rbind(total_error_four, d$two[2])

 }
 error_table[1,1] = mean(total_error_one)
error_table[1,2] = mean(total_error_two)
 error_table[1,3] = mean(total_error_three)
 error_table[1,4] = mean(total_error_four)

```

Now doing the same thing with the other data Sets next is the Hepatitis data Set 

```{r}
 colnames(hyp)[which(names(hyp) == "V20")] <- "class"
 n <- nrow(hyp)
 K <- 10
 rowsInFold <- n %/% K
 rang <- rank(runif(n))
 block <- (rang-1) %/% rowsInFold +1
 block <- as.factor(block)
```

<!--

```{r  eval=FALSE}

 total_error_one <- numeric(0)
  total_error_two <- numeric(0)
 total_error_three <- numeric(0)
 total_error_four <- numeric(0)

 for(k in 1:K) {
   sample(block)
   a <- cross_validate(hyp,block,rpart)
   b <- cross_validate(hyp,block,randomForest)
   c <- cross_validate(hyp,block,svm)
   d <- cross_validate(hyp,block,naiveBayes)

   total_error_one<- rbind(total_error_one, a$two[2])
    total_error_two<- rbind(total_error_two, b$two[2])
   total_error_three<- rbind(total_error_three, c$two[2])
   total_error_four<- rbind(total_error_four, d$two[2])

 }
 error_table[2,1] = mean(total_error_one)
error_table[2,2] = mean(total_error_two)
 error_table[2,3] = mean(total_error_three)
 error_table[2,4] = mean(total_error_four) 
```
--> 

Didnt have time to finish but proceed to fill the error_table as the previous example to fill the whole table and compare the results 
```{r}
error_table
```
